Flaky Rouge Bird

Medium

# Improper state machine will cause workflow ambiguity for project owners as completed deployments can unexpectedly return to in-progress status

### Summary

The improper state machine implementation will cause workflow ambiguity for project owners as completed deployments can unexpectedly return to in-progress status.


### Root Cause

In [BlueprintCore.sol:652-654](https://github.com/sherlock-audit/2025-03-crestal-network/blob/main/crestal-omni-contracts/src/BlueprintCore.sol#L652-L654), the contract allows transitioning backwards from a completed state to an in-progress state:

```solidity
// reset status if it is generated proof
if (requestDeploymentStatus[requestID].status == Status.GeneratedProof) {
    requestDeploymentStatus[requestID].status = Status.Pickup;
}
```

This code is found in the `updateDeploymentConfig` function, which allows a worker to modify the deployment configuration. If a deployment has reached the final state (`GeneratedProof`), the status will be reset to `Pickup` when the configuration is updated.

This creates a circular dependency in the state machine, undermining its integrity. State machines should generally follow a forward-only progression, especially for states representing completion of work. By allowing a completed deployment to revert to an in-progress state, the contract breaks the expected unidirectional flow and introduces ambiguity about which state represents the "true" status of a deployment.

The issue is further exacerbated because there is no versioning or tracking mechanism for these state transitions. When a deployment returns to the `Pickup` state from `GeneratedProof`, there is no record that it previously had a proof submitted, and the new proof submission will overwrite the old one. This allows a form of history erasure that creates ambiguity about which deployment proof is the "true" or "final" one.

A properly designed state machine would either:
1. Not allow reverting to previous states, or
2. Implement a versioning/revision system to track multiple iterations

Without either of these safeguards, the current implementation allows for proof replacement with no clear audit trail of changes.

### Internal Pre-conditions

1. A deployment request must exist (have been created through one of the deployment request creation functions)
2. The deployment request must have reached the `GeneratedProof` status (a worker has submitted proof of deployment)
3. The deployment request must be active
4. The worker assigned to the deployment must be the one making the update request


### External Pre-conditions

No special external conditions are needed - this issue exists entirely within the smart contract's logic. The issue can be triggered through normal, expected usage of the contract's functionality.


### Attack Path

1. A deployment request is created for a project
2. A worker is assigned and picks up the deployment request, changing its status to `Pickup`
3. The worker completes the deployment and submits proof, changing the status to `GeneratedProof`, which should represent the completed state
4. At this point, a project owner might consider the deployment finalized and make business decisions based on this status
5. Later, the same worker calls `updateDeploymentConfig` to modify the deployment configuration
6. During this update, the `GeneratedProof` status is reset to `Pickup`, indicating that the deployment is once again in progress
7. The deployment returns to an "in-progress" state despite having previously been marked as complete
8. The worker can now submit a completely different proof of deployment
9. The original proof is overwritten, and there's no record that it was ever different

This creates a "proof replacement attack" where a worker can effectively rewrite the history of what was deployed. A project owner who observed the deployment reached `GeneratedProof` would have no on-chain way to know that the proof was later changed.

### Impact

This circular state transition creates several significant problems:

- **Status Ambiguity**: Project owners cannot rely on the status to determine if a deployment is truly completed, as it may unexpectedly return to an in-progress state
- **Proof Invalidation**: Previously submitted and accepted proofs can be invalidated and replaced without a clear audit trail
- **History Manipulation**: The contract allows effectively rewriting the history of what was deployed without preserving the previous records
- **Trust Degradation**: Project owners cannot trust the finality of the `GeneratedProof` status, undermining confidence in the system
- **Business Logic Disruption**: Systems or processes that rely on the finality of the `GeneratedProof` state may malfunction when deployments unexpectedly return to `Pickup`
- **Unexpected State Changes**: Project owners may experience confusion when deployments they believed were completed suddenly return to in-progress
- **Audit Trail Concerns**: The ability to overwrite a previous proof without a trace makes auditing deployment history problematic
- **Governance Implications**: In systems with governance or dispute resolution, the lack of immutable proof records complicates fair resolution
- **Financial Implications**: If payment or rewards are tied to reaching the `GeneratedProof` state, the circular transition could lead to confusion about compensation

While this may not lead to direct fund loss, it significantly undermines the integrity of the deployment workflow and the trustworthiness of the system.

### PoC

```solidity
function demonstrateCircularTransition() public {
    // Setup: Create project and deployment request
    bytes32 projectId = createProjectID();
    bytes32 requestID = createDeploymentRequest(
        projectId,
        address(0),
        "Base64 encoded proposal",
        "https://server.url/api"
    );
    
    // Check initial state
    (BlueprintCore.Status status, address worker) = requestDeploymentStatus(requestID);
    assert(status == BlueprintCore.Status.Issued);
    console.log("Initial status:", uint256(status)); // Should be 1 (Issued)
    
    // Worker picks up the request
    address workerAddress = address(0xdeadbeef);
    vm.prank(workerAddress);
    pickupDeploymentRequest(projectId, requestID);
    
    // Check state after pickup
    (status, worker) = requestDeploymentStatus(requestID);
    assert(status == BlueprintCore.Status.Pickup);
    assert(worker == workerAddress);
    console.log("Status after pickup:", uint256(status)); // Should be 2 (Pickup)
    
    // Worker submits proof of deployment
    string memory originalProof = "Original deployment proof - hash: 0x123456";
    vm.prank(workerAddress);
    submitProofOfDeployment(projectId, requestID, originalProof);
    
    // Check state after proof submission
    (status, worker) = requestDeploymentStatus(requestID);
    assert(status == BlueprintCore.Status.GeneratedProof);
    console.log("Status after proof submission:", uint256(status)); // Should be 5 (GeneratedProof)
    
    // Verify the proof is stored correctly
    string memory storedProof = deploymentProof[requestID];
    assert(keccak256(bytes(storedProof)) == keccak256(bytes(originalProof)));
    console.log("Original proof stored successfully");
    
    // Now, worker updates the deployment configuration
    string memory updatedConfig = "Updated configuration";
    vm.prank(workerAddress);
    updateDeploymentConfig(projectId, requestID, updatedConfig);
    
    // Check state after update - should have reverted to Pickup
    (status, worker) = requestDeploymentStatus(requestID);
    assert(status == BlueprintCore.Status.Pickup);
    console.log("Status after config update:", uint256(status)); // Should be 2 (Pickup)
    console.log("!!! Status reverted from GeneratedProof (5) back to Pickup (2) !!!");
    
    // Worker can now submit a completely different proof
    string memory newProof = "Completely different proof - hash: 0xabcdef";
    vm.prank(workerAddress);
    submitProofOfDeployment(projectId, requestID, newProof);
    
    // Check final state
    (status, worker) = requestDeploymentStatus(requestID);
    assert(status == BlueprintCore.Status.GeneratedProof);
    console.log("Status after new proof submission:", uint256(status)); // Should be 5 (GeneratedProof)
    
    // Verify the original proof has been overwritten
    storedProof = deploymentProof[requestID];
    assert(keccak256(bytes(storedProof)) == keccak256(bytes(newProof)));
    assert(keccak256(bytes(storedProof)) != keccak256(bytes(originalProof)));
    console.log("Original proof has been overwritten with new proof");
    console.log("No record of original proof exists on-chain");
}
```

This test demonstrates that a deployment can progress through the normal workflow to reach `GeneratedProof` (status 5), but then be reverted to `Pickup` (status 2) through a configuration update. The worker can then submit a completely different proof, which overwrites the original, with no trace of the original proof remaining in the contract.

### Mitigation

We recommend one of the following approaches to address this issue:

### Option 1: Remove the Circular Transition
The simplest solution is to remove the status reset in the `updateDeploymentConfig` function:

```solidity
function updateDeploymentConfig(bytes32 projectId, bytes32 requestID, string memory updatedBase64Config)
    public
    hasProject(projectId)
{
    require(requestID.length > 0, "requestID is empty");
    require(requestDeploymentStatus[requestID].status != Status.Init, "requestID does not exist");
    address workerAddr = requestDeploymentStatus[requestID].deployWorkerAddr;
    require(workerAddr == msg.sender, "Wrong worker address");
    
    // REMOVE THIS BLOCK - Don't reset status when a deployment is already complete
    // if (requestDeploymentStatus[requestID].status == Status.GeneratedProof) {
    //     requestDeploymentStatus[requestID].status = Status.Pickup;
    // }
    
    // Instead, store updates without changing status of completed deployments
    if (requestDeploymentStatus[requestID].status == Status.GeneratedProof) {
        deploymentUpdates[requestID].push(updatedBase64Config);
        emit DeploymentConfigUpdated(projectId, requestID, updatedBase64Config);
    } else {
        emit UpdateDeploymentConfig(projectId, requestID, workerAddr, updatedBase64Config);
    }
}
```

### Option 2: Implement a Versioning System
If updates to completed deployments need to be tracked, implement a versioning system for proofs:

```solidity
// Add a struct for versioned proofs
struct DeploymentProofVersion {
    string proofBase64;
    uint256 timestamp;
    uint256 version;
}

// Map deployments to their versioned proofs
mapping(bytes32 => DeploymentProofVersion[]) public deploymentProofVersions;
mapping(bytes32 => uint256) public currentProofVersion;

// Modified submitProofOfDeployment to handle versioning
function submitProofOfDeployment(bytes32 projectId, bytes32 requestID, string memory proofBase64)
    public
    hasProject(projectId)
{
    require(requestID.length > 0, "requestID is empty");
    require(requestDeploymentStatus[requestID].status != Status.Init, "requestID does not exist");
    require(requestDeploymentStatus[requestID].deployWorkerAddr == msg.sender, "Wrong worker address");
    
    uint256 newVersion;
    
    // If this is a new proof (not an update)
    if (requestDeploymentStatus[requestID].status != Status.GeneratedProof) {
        // Set status to GeneratedProof
        requestDeploymentStatus[requestID].status = Status.GeneratedProof;
        newVersion = 1;
    } else {
        // This is an update to an existing proof
        newVersion = currentProofVersion[requestID] + 1;
    }
    
    // Store the new versioned proof
    deploymentProofVersions[requestID].push(DeploymentProofVersion({
        proofBase64: proofBase64,
        timestamp: block.timestamp,
        version: newVersion
    }));
    
    // Update current version
    currentProofVersion[requestID] = newVersion;
    
    // Save to the legacy mapping for backwards compatibility
    deploymentProof[requestID] = proofBase64;
    
    emit GeneratedProofOfDeploymentVersion(projectId, requestID, proofBase64, newVersion);
}

// Modified updateDeploymentConfig function
function updateDeploymentConfig(bytes32 projectId, bytes32 requestID, string memory updatedBase64Config)
    public
    hasProject(projectId)
{
    require(requestID.length > 0, "requestID is empty");
    require(requestDeploymentStatus[requestID].status != Status.Init, "requestID does not exist");
    address workerAddr = requestDeploymentStatus[requestID].deployWorkerAddr;
    require(workerAddr == msg.sender, "Wrong worker address");
    
    // Don't reset status for completed deployments
    // Instead, track the config update separately
    if (requestDeploymentStatus[requestID].status == Status.GeneratedProof) {
        deploymentUpdates[requestID].push(updatedBase64Config);
        emit DeploymentConfigUpdated(projectId, requestID, updatedBase64Config);
    } else {
        emit UpdateDeploymentConfig(projectId, requestID, workerAddr, updatedBase64Config);
    }
}
```

### Option 3: Add a New State for Updated Deployments
Create a new state that indicates a deployment has been completed but subsequently updated:

```solidity
enum Status {
    Init,
    Issued,
    Pickup,
    Deploying,
    Deployed,
    GeneratedProof,
    UpdatedAfterProof  // New state for deployments that have been updated after proof
}

function updateDeploymentConfig(bytes32 projectId, bytes32 requestID, string memory updatedBase64Config)
    public
    hasProject(projectId)
{
    require(requestID.length > 0, "requestID is empty");
    require(requestDeploymentStatus[requestID].status != Status.Init, "requestID does not exist");
    address workerAddr = requestDeploymentStatus[requestID].deployWorkerAddr;
    require(workerAddr == msg.sender, "Wrong worker address");
    
    // Instead of resetting to Pickup, transition to a new state
    if (requestDeploymentStatus[requestID].status == Status.GeneratedProof) {
        requestDeploymentStatus[requestID].status = Status.UpdatedAfterProof;
        
        // Store the update in the updates array
        deploymentUpdates[requestID].push(updatedBase64Config);
        emit DeploymentUpdatedAfterProof(projectId, requestID, updatedBase64Config);
    } else {
        emit UpdateDeploymentConfig(projectId, requestID, workerAddr, updatedBase64Config);
    }
}
```

### Recommendation
The preferred solution is Option 2 (implement a versioning system). This approach preserves the complete history of proofs while allowing for updates to completed deployments. It maintains backward compatibility and provides a clear audit trail of all changes to a deployment's proof.

If a simpler solution is needed, Option 1 (remove the circular transition) would be sufficient to address the immediate issue, but would not allow for updated proofs at all. Option 3 (add a new state) is a compromise that preserves more information than the current implementation but doesn't provide the full versioning of Option 2.
